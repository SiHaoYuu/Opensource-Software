import requests
import json
import time
from typing import Dict, List, Optional, Any
import pandas as pd
from datetime import datetime
import csv
import os


class MaxDataVSCodeCrawler:
    """
    ä¿®å¤ç‰ˆVS Codeå¤§æ•°æ®çˆ¬è™«
    ä¸“é—¨ä¿®å¤NoneTypeé”™è¯¯
    """

    def __init__(self, github_token: str = None):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            github_token: GitHub Personal Access Tokenï¼ˆå¿…é¡»ï¼ï¼‰
        """
        if not github_token or github_token == "ghp_your_token_here":
            print("âš ï¸  è­¦å‘Šï¼šè·å–å¤§é‡æ•°æ®å¿…é¡»ä½¿ç”¨GitHub Tokenï¼")
            print("è¯·å…ˆåœ¨ https://github.com/settings/tokens åˆ›å»ºtoken")
            print("å¹¶æ›¿æ¢ä»£ç ä¸­çš„ GITHUB_TOKEN å˜é‡")
            raise ValueError("éœ€è¦GitHub Token")

        self.github_token = github_token
        self.base_url = "https://api.github.com/repos/microsoft/vscode"
        self.session = requests.Session()

        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            "Authorization": f"token {self.github_token}",
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "Mozilla/5.0 (compatible; VSCodeMaxCrawler/1.0)"
        }

        self.session.headers.update(self.headers)
        self.max_per_page = 100  # GitHubæ¯é¡µæœ€å¤§100æ¡

        # é…ç½®è·å–çš„æœ€å¤§æ•°é‡ï¼ˆæ›´ä¿å®ˆçš„è®¾ç½®ï¼‰
        self.config = {
            'contributors': 300,  # è´¡çŒ®è€…ï¼š300æ¡
            'commits': 500,  # æäº¤ï¼š500æ¡
            'issues': 200,  # é—®é¢˜ï¼š200æ¡
            'prs': 200,  # PRï¼š200æ¡
            'releases': 50,  # å‘å¸ƒï¼š50æ¡
            'branches': 30,  # åˆ†æ”¯ï¼š30æ¡
            'stargazers': 300,  # Starç”¨æˆ·ï¼š300æ¡
            'forks': 100,  # Forkä»“åº“ï¼š100æ¡
        }

    def _make_request_safe(self, url: str, params: Dict = None) -> Optional[Any]:
        """
        å®‰å…¨çš„APIè¯·æ±‚ï¼Œå¢åŠ é‡è¯•æœºåˆ¶
        """
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, params=params, timeout=30)

                # æ˜¾ç¤ºAPIé™åˆ¶
                remaining = response.headers.get('X-RateLimit-Remaining', 'N/A')
                limit = response.headers.get('X-RateLimit-Limit', 'N/A')
                print(f"ğŸ“Š APIå‰©ä½™: {remaining}/{limit}")

                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 403:
                    reset_time = response.headers.get('X-RateLimit-Reset')
                    if reset_time:
                        reset_time = datetime.fromtimestamp(int(reset_time))
                        wait_seconds = max(10, (reset_time - datetime.now()).total_seconds())
                        print(f"â° APIé™åˆ¶ï¼Œç­‰å¾… {wait_seconds:.0f} ç§’...")
                        time.sleep(wait_seconds + 2)
                        continue
                    else:
                        print("âŒ æœªçŸ¥çš„403é”™è¯¯")
                        return None
                elif response.status_code in [404, 422]:
                    print(f"âŒ {response.status_code}: {response.text[:100]}")
                    return None
                else:
                    print(f"âŒ é”™è¯¯ {response.status_code}")
                    return None

            except requests.exceptions.Timeout:
                print(f"â±ï¸  è¯·æ±‚è¶…æ—¶ï¼Œå°è¯• {attempt + 1}/{max_retries}")
                time.sleep(5)
            except Exception as e:
                print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
                time.sleep(3)

        print(f"âš ï¸  è¯·æ±‚å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡")
        return None

    def _safe_len(self, obj) -> int:
        """å®‰å…¨çš„è·å–é•¿åº¦ï¼Œå¤„ç†Noneå€¼"""
        if obj is None:
            return 0
        try:
            return len(obj)
        except:
            return 0

    def _safe_get(self, data: Dict, key: str, default: Any = ''):
        """å®‰å…¨è·å–å­—å…¸å€¼"""
        if not isinstance(data, dict):
            return default

        value = data.get(key, default)
        if value is None:
            return default
        return value

    def get_massive_contributors(self) -> List[Dict]:
        """
        è·å–å¤§é‡è´¡çŒ®è€…æ•°æ®
        """
        print(f"ğŸ” è·å–è´¡çŒ®è€…æ•°æ®ï¼ˆç›®æ ‡: {self.config['contributors']}æ¡ï¼‰...")

        contributors = []
        page = 1

        while len(contributors) < self.config['contributors']:
            print(f"  è·å–ç¬¬{page}é¡µè´¡çŒ®è€…...")

            params = {
                "per_page": min(self.max_per_page, self.config['contributors'] - len(contributors)),
                "page": page,
            }

            url = f"{self.base_url}/contributors"
            data = self._make_request_safe(url, params)

            if data is None or not isinstance(data, list):
                print("  âš ï¸  è·å–æ•°æ®å¤±è´¥æˆ–æ ¼å¼é”™è¯¯")
                break

            if len(data) == 0:
                print(f"  âœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼Œå…± {len(contributors)} æ¡")
                break

            for item in data:
                if not isinstance(item, dict):
                    continue

                contributors.append({
                    "åºå·": len(contributors) + 1,
                    "ç”¨æˆ·å": self._safe_get(item, 'login', 'æœªçŸ¥'),
                    "è´¡çŒ®æ¬¡æ•°": self._safe_get(item, 'contributions', 0),
                    "ç”¨æˆ·ID": self._safe_get(item, 'id', ''),
                    "å¤´åƒURL": self._safe_get(item, 'avatar_url', ''),
                    "ä¸»é¡µ": self._safe_get(item, 'html_url', ''),
                    "ç±»å‹": self._safe_get(item, 'type', 'User'),
                    "ç®¡ç†å‘˜": self._safe_get(item, 'site_admin', False),
                    "è·å–æ—¶é—´": datetime.now().isoformat(),
                    "é¡µç ": page
                })

            if len(data) < params["per_page"]:
                print(f"  âœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼Œå…± {len(contributors)} æ¡")
                break

            time.sleep(0.8)
            page += 1

            if len(contributors) >= self.config['contributors']:
                print(f"  âœ… å·²è¾¾åˆ°ç›®æ ‡æ•°é‡: {len(contributors)} æ¡")
                break

        print(f"âœ… æœ€ç»ˆè·å–åˆ° {len(contributors)} æ¡è´¡çŒ®è€…æ•°æ®")
        return contributors

    def get_massive_commits(self, since_date: str = "2023-01-01") -> List[Dict]:
        """
        è·å–å¤§é‡æäº¤è®°å½•
        """
        print(f"ğŸ” è·å–æäº¤è®°å½•ï¼ˆç›®æ ‡: {self.config['commits']}æ¡ï¼‰...")

        commits = []
        page = 1

        while len(commits) < self.config['commits']:
            print(f"  è·å–ç¬¬{page}é¡µæäº¤è®°å½•...")

            params = {
                "per_page": min(self.max_per_page, self.config['commits'] - len(commits)),
                "page": page,
                "since": since_date
            }

            url = f"{self.base_url}/commits"
            data = self._make_request_safe(url, params)

            if data is None or not isinstance(data, list):
                print("  âš ï¸  è·å–æ•°æ®å¤±è´¥æˆ–æ ¼å¼é”™è¯¯")
                break

            if len(data) == 0:
                print(f"  âœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼Œå…± {len(commits)} æ¡")
                break

            for commit in data:
                if not isinstance(commit, dict):
                    continue

                commit_info = self._safe_get(commit, 'commit', {})
                author_info = self._safe_get(commit_info, 'author', {})

                commits.append({
                    "åºå·": len(commits) + 1,
                    "SHA": self._safe_get(commit, 'sha', ''),
                    "çŸ­SHA": self._safe_get(commit, 'sha', '')[:8],
                    "æäº¤ä¿¡æ¯": self._safe_get(commit_info, 'message', '')[:200],
                    "ä½œè€…": self._safe_get(author_info, 'name', ''),
                    "ä½œè€…é‚®ç®±": self._safe_get(author_info, 'email', ''),
                    "æäº¤æ—¶é—´": self._safe_get(author_info, 'date', ''),
                    "GitHubç”¨æˆ·": self._safe_get(self._safe_get(commit, 'author', {}), 'login', ''),
                    "URL": self._safe_get(commit, 'html_url', ''),
                    "è·å–æ—¶é—´": datetime.now().isoformat(),
                    "é¡µç ": page
                })

            if len(data) < params["per_page"]:
                print(f"  âœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼Œå…± {len(commits)} æ¡")
                break

            time.sleep(0.8)
            page += 1

            if len(commits) >= self.config['commits']:
                print(f"  âœ… å·²è¾¾åˆ°ç›®æ ‡æ•°é‡: {len(commits)} æ¡")
                break

        print(f"âœ… æœ€ç»ˆè·å–åˆ° {len(commits)} æ¡æäº¤è®°å½•")
        return commits

    def get_massive_issues_safe(self, state: str = "all", issue_type: str = "issues") -> List[Dict]:
        """
        ä¿®å¤ç‰ˆï¼šè·å–å¤§é‡é—®é¢˜/PRæ•°æ®ï¼ˆä¿®å¤NoneTypeé”™è¯¯ï¼‰
        """
        max_items = self.config['issues'] if issue_type == "issues" else self.config['prs']
        type_name = "é—®é¢˜" if issue_type == "issues" else "PR"

        print(f"ğŸ” è·å–{state}{type_name}ï¼ˆç›®æ ‡: {max_items}æ¡ï¼‰...")

        items = []
        page = 1
        endpoint = "/issues" if issue_type == "issues" else "/pulls"

        while len(items) < max_items:
            print(f"  è·å–ç¬¬{page}é¡µ{type_name}...")

            params = {
                "per_page": min(self.max_per_page, max_items - len(items)),
                "page": page,
                "state": state,
            }

            url = f"{self.base_url}{endpoint}"
            data = self._make_request_safe(url, params)

            if data is None or not isinstance(data, list):
                print("  âš ï¸  è·å–æ•°æ®å¤±è´¥æˆ–æ ¼å¼é”™è¯¯")
                break

            if len(data) == 0:
                print(f"  âœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼Œå…± {len(items)} æ¡")
                break

            for item in data:
                if not isinstance(item, dict):
                    continue

                # å¯¹äºissuesç«¯ç‚¹ï¼Œéœ€è¦è¿‡æ»¤æ‰PR
                if endpoint == "/issues" and 'pull_request' in item:
                    continue

                # å®‰å…¨è·å–æ‰€æœ‰å­—æ®µ
                body = self._safe_get(item, 'body', '')
                labels = self._safe_get(item, 'labels', [])
                user_info = self._safe_get(item, 'user', {})

                # å¤„ç†æ ‡ç­¾
                label_names = []
                if isinstance(labels, list):
                    for label in labels:
                        if isinstance(label, dict):
                            name = self._safe_get(label, 'name', '')
                            if name:
                                label_names.append(name)

                items.append({
                    "åºå·": len(items) + 1,
                    "ç¼–å·": self._safe_get(item, 'number', 0),
                    "æ ‡é¢˜": self._safe_get(item, 'title', ''),
                    "ç±»å‹": "PR" if 'pull_request' in item else "Issue",
                    "çŠ¶æ€": self._safe_get(item, 'state', ''),
                    "åˆ›å»ºè€…": self._safe_get(user_info, 'login', ''),
                    "åˆ›å»ºæ—¶é—´": self._safe_get(item, 'created_at', ''),
                    "æ›´æ–°æ—¶é—´": self._safe_get(item, 'updated_at', ''),
                    "å…³é—­æ—¶é—´": self._safe_get(item, 'closed_at', ''),
                    "æ ‡ç­¾æ•°": len(label_names),
                    "æ ‡ç­¾": ', '.join(label_names[:3]),  # åªå–å‰3ä¸ªæ ‡ç­¾
                    "è¯„è®ºæ•°": self._safe_get(item, 'comments', 0),
                    "æ­£æ–‡é•¿åº¦": self._safe_len(body),  # ä½¿ç”¨å®‰å…¨é•¿åº¦å‡½æ•°
                    "æ­£æ–‡é¢„è§ˆ": body[:100] + "..." if body else '',
                    "URL": self._safe_get(item, 'html_url', ''),
                    "è·å–æ—¶é—´": datetime.now().isoformat(),
                    "é¡µç ": page
                })

            print(f"  æœ¬é¡µè·å–: {len(data)} æ¡ï¼Œç´¯è®¡: {len(items)} æ¡")

            if len(data) < params["per_page"]:
                print(f"  âœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼Œå…± {len(items)} æ¡")
                break

            time.sleep(1.2)  # Issues APIé™åˆ¶è¾ƒä¸¥æ ¼
            page += 1

            if len(items) >= max_items:
                print(f"  âœ… å·²è¾¾åˆ°ç›®æ ‡æ•°é‡: {len(items)} æ¡")
                break

        print(f"âœ… æœ€ç»ˆè·å–åˆ° {len(items)} æ¡{type_name}æ•°æ®")
        return items

    def get_massive_stargazers(self) -> List[Dict]:
        """
        è·å–å¤§é‡starç”¨æˆ·
        """
        print(f"ğŸ” è·å–Starç”¨æˆ·ï¼ˆç›®æ ‡: {self.config['stargazers']}æ¡ï¼‰...")

        stargazers = []
        page = 1

        while len(stargazers) < self.config['stargazers']:
            print(f"  è·å–ç¬¬{page}é¡µStarç”¨æˆ·...")

            params = {
                "per_page": min(self.max_per_page, self.config['stargazers'] - len(stargazers)),
                "page": page
            }

            url = f"{self.base_url}/stargazers"
            data = self._make_request_safe(url, params)

            if data is None or not isinstance(data, list):
                print("  âš ï¸  è·å–æ•°æ®å¤±è´¥æˆ–æ ¼å¼é”™è¯¯")
                break

            if len(data) == 0:
                print(f"  âœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼Œå…± {len(stargazers)} æ¡")
                break

            for user in data:
                if not isinstance(user, dict):
                    continue

                stargazers.append({
                    "åºå·": len(stargazers) + 1,
                    "ç”¨æˆ·å": self._safe_get(user, 'login', 'æœªçŸ¥'),
                    "ç”¨æˆ·ID": self._safe_get(user, 'id', ''),
                    "å¤´åƒURL": self._safe_get(user, 'avatar_url', ''),
                    "ä¸»é¡µ": self._safe_get(user, 'html_url', ''),
                    "ç±»å‹": self._safe_get(user, 'type', 'User'),
                    "ç®¡ç†å‘˜": self._safe_get(user, 'site_admin', False),
                    "è·å–æ—¶é—´": datetime.now().isoformat(),
                    "é¡µç ": page
                })

            if len(data) < params["per_page"]:
                print(f"  âœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼Œå…± {len(stargazers)} æ¡")
                break

            time.sleep(1.0)
            page += 1

            if len(stargazers) >= self.config['stargazers']:
                print(f"  âœ… å·²è¾¾åˆ°ç›®æ ‡æ•°é‡: {len(stargazers)} æ¡")
                break

        print(f"âœ… æœ€ç»ˆè·å–åˆ° {len(stargazers)} æ¡Starç”¨æˆ·æ•°æ®")
        return stargazers

    def get_massive_forks(self) -> List[Dict]:
        """
        è·å–å¤§é‡forkä¿¡æ¯
        """
        print(f"ğŸ” è·å–Forkä»“åº“ï¼ˆç›®æ ‡: {self.config['forks']}æ¡ï¼‰...")

        forks = []
        page = 1

        while len(forks) < self.config['forks']:
            print(f"  è·å–ç¬¬{page}é¡µFork...")

            params = {
                "per_page": min(self.max_per_page, self.config['forks'] - len(forks)),
                "page": page,
            }

            url = f"{self.base_url}/forks"
            data = self._make_request_safe(url, params)

            if data is None or not isinstance(data, list):
                print("  âš ï¸  è·å–æ•°æ®å¤±è´¥æˆ–æ ¼å¼é”™è¯¯")
                break

            if len(data) == 0:
                print(f"  âœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼Œå…± {len(forks)} æ¡")
                break

            for fork in data:
                if not isinstance(fork, dict):
                    continue

                owner_info = self._safe_get(fork, 'owner', {})
                description = self._safe_get(fork, 'description', '')

                forks.append({
                    "åºå·": len(forks) + 1,
                    "ä»“åº“å": self._safe_get(fork, 'full_name', ''),
                    "æ‰€æœ‰è€…": self._safe_get(owner_info, 'login', ''),
                    "æ˜¯å¦ç§æœ‰": self._safe_get(fork, 'private', False),
                    "æè¿°": description[:150] if description else '',
                    "Forkæ—¶é—´": self._safe_get(fork, 'created_at', ''),
                    "æ›´æ–°æ—¶é—´": self._safe_get(fork, 'updated_at', ''),
                    "æ¨é€æ—¶é—´": self._safe_get(fork, 'pushed_at', ''),
                    "Starsæ•°": self._safe_get(fork, 'stargazers_count', 0),
                    "è¯­è¨€": self._safe_get(fork, 'language', ''),
                    "ä¸»é¡µ": self._safe_get(fork, 'html_url', ''),
                    "è·å–æ—¶é—´": datetime.now().isoformat(),
                    "é¡µç ": page
                })

            if len(data) < params["per_page"]:
                print(f"  âœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼Œå…± {len(forks)} æ¡")
                break

            time.sleep(0.8)
            page += 1

            if len(forks) >= self.config['forks']:
                print(f"  âœ… å·²è¾¾åˆ°ç›®æ ‡æ•°é‡: {len(forks)} æ¡")
                break

        print(f"âœ… æœ€ç»ˆè·å–åˆ° {len(forks)} æ¡Forkæ•°æ®")
        return forks

    def get_massive_branches(self) -> List[Dict]:
        """
        è·å–æ‰€æœ‰åˆ†æ”¯
        """
        print(f"ğŸ” è·å–åˆ†æ”¯åˆ—è¡¨ï¼ˆç›®æ ‡: {self.config['branches']}æ¡ï¼‰...")

        branches = []
        page = 1

        while len(branches) < self.config['branches']:
            print(f"  è·å–ç¬¬{page}é¡µåˆ†æ”¯...")

            params = {
                "per_page": min(self.max_per_page, self.config['branches'] - len(branches)),
                "page": page
            }

            url = f"{self.base_url}/branches"
            data = self._make_request_safe(url, params)

            if data is None or not isinstance(data, list):
                print("  âš ï¸  è·å–æ•°æ®å¤±è´¥æˆ–æ ¼å¼é”™è¯¯")
                break

            if len(data) == 0:
                print(f"  âœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼Œå…± {len(branches)} æ¡")
                break

            for branch in data:
                if not isinstance(branch, dict):
                    continue

                commit_info = self._safe_get(branch, 'commit', {})

                branches.append({
                    "åºå·": len(branches) + 1,
                    "åˆ†æ”¯å": self._safe_get(branch, 'name', ''),
                    "æ˜¯å¦å—ä¿æŠ¤": self._safe_get(branch, 'protected', False),
                    "æäº¤SHA": self._safe_get(commit_info, 'sha', ''),
                    "è·å–æ—¶é—´": datetime.now().isoformat(),
                    "é¡µç ": page
                })

            if len(data) < params["per_page"]:
                print(f"  âœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼Œå…± {len(branches)} æ¡")
                break

            time.sleep(0.5)
            page += 1

            if len(branches) >= self.config['branches']:
                print(f"  âœ… å·²è¾¾åˆ°ç›®æ ‡æ•°é‡: {len(branches)} æ¡")
                break

        print(f"âœ… æœ€ç»ˆè·å–åˆ° {len(branches)} æ¡åˆ†æ”¯æ•°æ®")
        return branches

    def get_massive_releases(self) -> List[Dict]:
        """
        è·å–æ‰€æœ‰å‘å¸ƒç‰ˆæœ¬
        """
        print(f"ğŸ” è·å–å‘å¸ƒç‰ˆæœ¬ï¼ˆç›®æ ‡: {self.config['releases']}æ¡ï¼‰...")

        releases = []
        page = 1

        while len(releases) < self.config['releases']:
            print(f"  è·å–ç¬¬{page}é¡µå‘å¸ƒç‰ˆæœ¬...")

            params = {
                "per_page": min(self.max_per_page, self.config['releases'] - len(releases)),
                "page": page
            }

            url = f"{self.base_url}/releases"
            data = self._make_request_safe(url, params)

            if data is None or not isinstance(data, list):
                print("  âš ï¸  è·å–æ•°æ®å¤±è´¥æˆ–æ ¼å¼é”™è¯¯")
                break

            if len(data) == 0:
                print(f"  âœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼Œå…± {len(releases)} æ¡")
                break

            for release in data:
                if not isinstance(release, dict):
                    continue

                body = self._safe_get(release, 'body', '')
                author_info = self._safe_get(release, 'author', {})
                assets = self._safe_get(release, 'assets', [])

                # è®¡ç®—æ€»ä¸‹è½½é‡
                total_downloads = 0
                if isinstance(assets, list):
                    for asset in assets:
                        if isinstance(asset, dict):
                            total_downloads += self._safe_get(asset, 'download_count', 0)

                releases.append({
                    "åºå·": len(releases) + 1,
                    "ç‰ˆæœ¬å·": self._safe_get(release, 'tag_name', ''),
                    "ç‰ˆæœ¬åç§°": self._safe_get(release, 'name', ''),
                    "å‘å¸ƒè€…": self._safe_get(author_info, 'login', ''),
                    "å‘å¸ƒæ—¥æœŸ": self._safe_get(release, 'published_at', ''),
                    "é¢„å‘å¸ƒ": self._safe_get(release, 'prerelease', False),
                    "è‰ç¨¿": self._safe_get(release, 'draft', False),
                    "å‘å¸ƒè¯´æ˜é•¿åº¦": self._safe_len(body),
                    "å‘å¸ƒè¯´æ˜é¢„è§ˆ": body[:120] + "..." if body else '',
                    "èµ„äº§æ•°é‡": len(assets) if isinstance(assets, list) else 0,
                    "æ€»ä¸‹è½½é‡": total_downloads,
                    "URL": self._safe_get(release, 'html_url', ''),
                    "è·å–æ—¶é—´": datetime.now().isoformat(),
                    "é¡µç ": page
                })

            if len(data) < params["per_page"]:
                print(f"  âœ… å·²è·å–æ‰€æœ‰æ•°æ®ï¼Œå…± {len(releases)} æ¡")
                break

            time.sleep(0.8)
            page += 1

            if len(releases) >= self.config['releases']:
                print(f"  âœ… å·²è¾¾åˆ°ç›®æ ‡æ•°é‡: {len(releases)} æ¡")
                break

        print(f"âœ… æœ€ç»ˆè·å–åˆ° {len(releases)} æ¡å‘å¸ƒæ•°æ®")
        return releases

    def get_repository_stats(self) -> Dict[str, Any]:
        """è·å–ä»“åº“ç»Ÿè®¡ä¿¡æ¯"""
        print("ğŸ” è·å–ä»“åº“ç»Ÿè®¡ä¿¡æ¯...")

        url = f"{self.base_url}"
        data = self._make_request_safe(url)

        if isinstance(data, dict):
            license_info = self._safe_get(data, 'license', {})

            return {
                "æ€»Stars": self._safe_get(data, 'stargazers_count', 0),
                "æ€»Forks": self._safe_get(data, 'forks_count', 0),
                "æ€»Watchers": self._safe_get(data, 'watchers_count', 0),
                "å¼€æ”¾é—®é¢˜": self._safe_get(data, 'open_issues_count', 0),
                "ä»“åº“å¤§å°": self._safe_get(data, 'size', 0),
                "åˆ›å»ºæ—¶é—´": self._safe_get(data, 'created_at', ''),
                "æœ€åæ›´æ–°": self._safe_get(data, 'updated_at', ''),
                "æœ€åæ¨é€": self._safe_get(data, 'pushed_at', ''),
                "é»˜è®¤åˆ†æ”¯": self._safe_get(data, 'default_branch', 'main'),
                "è¯­è¨€": self._safe_get(data, 'language', ''),
                "License": self._safe_get(license_info, 'name', ''),
                "è·å–æ—¶é—´": datetime.now().isoformat()
            }
        return {}

    def export_massive_data_safely(self, export_dir: str = "vscode_massive_data"):
        """
        å®‰å…¨çš„å¯¼å‡ºå¤§é‡æ•°æ®ï¼ˆä¿®å¤ç‰ˆï¼‰
        """
        os.makedirs(export_dir, exist_ok=True)
        print(f"ğŸ“‚ æ•°æ®å°†å¯¼å‡ºåˆ°: {os.path.abspath(export_dir)}/")

        total_items = 0
        failed_apis = []

        try:
            # 1. è´¡çŒ®è€…
            print(f"\n{'=' * 60}")
            print("1. è·å–è´¡çŒ®è€…æ•°æ®...")
            contributors = self.get_massive_contributors()
            if contributors:
                success = self._export_to_csv_safe(contributors, f"{export_dir}/1_contributors.csv")
                if success:
                    total_items += len(contributors)
                else:
                    failed_apis.append("contributors")
            else:
                print("  âš ï¸  æœªè·å–åˆ°è´¡çŒ®è€…æ•°æ®")
                failed_apis.append("contributors")

            # 2. æäº¤è®°å½•
            print(f"\n{'=' * 60}")
            print("2. è·å–æäº¤è®°å½•...")
            commits = self.get_massive_commits()
            if commits:
                success = self._export_to_csv_safe(commits, f"{export_dir}/2_commits.csv")
                if success:
                    total_items += len(commits)
                else:
                    failed_apis.append("commits")
            else:
                print("  âš ï¸  æœªè·å–åˆ°æäº¤è®°å½•")
                failed_apis.append("commits")

            # 3. é—®é¢˜
            print(f"\n{'=' * 60}")
            print("3. è·å–é—®é¢˜æ•°æ®...")
            issues = self.get_massive_issues_safe(state="open", issue_type="issues")
            if issues:
                success = self._export_to_csv_safe(issues, f"{export_dir}/3_issues_open.csv")
                if success:
                    total_items += len(issues)
                else:
                    failed_apis.append("issues")
            else:
                print("  âš ï¸  æœªè·å–åˆ°é—®é¢˜æ•°æ®")
                failed_apis.append("issues")

            # 4. PR
            print(f"\n{'=' * 60}")
            print("4. è·å–PRæ•°æ®...")
            prs = self.get_massive_issues_safe(state="open", issue_type="pulls")
            if prs:
                success = self._export_to_csv_safe(prs, f"{export_dir}/4_prs_open.csv")
                if success:
                    total_items += len(prs)
                else:
                    failed_apis.append("prs")
            else:
                print("  âš ï¸  æœªè·å–åˆ°PRæ•°æ®")
                failed_apis.append("prs")

            # 5. Starç”¨æˆ·
            print(f"\n{'=' * 60}")
            print("5. è·å–Starç”¨æˆ·æ•°æ®...")
            stargazers = self.get_massive_stargazers()
            if stargazers:
                success = self._export_to_csv_safe(stargazers, f"{export_dir}/5_stargazers.csv")
                if success:
                    total_items += len(stargazers)
                else:
                    failed_apis.append("stargazers")
            else:
                print("  âš ï¸  æœªè·å–åˆ°Starç”¨æˆ·æ•°æ®")
                failed_apis.append("stargazers")

            # 6. Forkä»“åº“
            print(f"\n{'=' * 60}")
            print("6. è·å–Forkä»“åº“æ•°æ®...")
            forks = self.get_massive_forks()
            if forks:
                success = self._export_to_csv_safe(forks, f"{export_dir}/6_forks.csv")
                if success:
                    total_items += len(forks)
                else:
                    failed_apis.append("forks")
            else:
                print("  âš ï¸  æœªè·å–åˆ°Forkæ•°æ®")
                failed_apis.append("forks")

            # 7. å‘å¸ƒç‰ˆæœ¬
            print(f"\n{'=' * 60}")
            print("7. è·å–å‘å¸ƒç‰ˆæœ¬æ•°æ®...")
            releases = self.get_massive_releases()
            if releases:
                success = self._export_to_csv_safe(releases, f"{export_dir}/7_releases.csv")
                if success:
                    total_items += len(releases)
                else:
                    failed_apis.append("releases")
            else:
                print("  âš ï¸  æœªè·å–åˆ°å‘å¸ƒæ•°æ®")
                failed_apis.append("releases")

            # 8. åˆ†æ”¯
            print(f"\n{'=' * 60}")
            print("8. è·å–åˆ†æ”¯æ•°æ®...")
            branches = self.get_massive_branches()
            if branches:
                success = self._export_to_csv_safe(branches, f"{export_dir}/8_branches.csv")
                if success:
                    total_items += len(branches)
                else:
                    failed_apis.append("branches")
            else:
                print("  âš ï¸  æœªè·å–åˆ°åˆ†æ”¯æ•°æ®")
                failed_apis.append("branches")

            # 9. ä»“åº“ç»Ÿè®¡
            print(f"\n{'=' * 60}")
            print("9. è·å–ä»“åº“ç»Ÿè®¡ä¿¡æ¯...")
            stats = self.get_repository_stats()
            if stats:
                self._export_to_csv_safe([stats], f"{export_dir}/9_repository_stats.csv")

            # æ˜¾ç¤ºæ€»ç»“
            print(f"\n{'=' * 60}")
            print("ğŸ“Š æ•°æ®è·å–æ€»ç»“")
            print("=" * 60)
            print(f"âœ… æˆåŠŸè·å–æ€»æ•°æ®æ¡æ•°: {total_items:,}")

            if failed_apis:
                print(f"âš ï¸  ä»¥ä¸‹APIè·å–å¤±è´¥: {', '.join(failed_apis)}")

            print(f"\nğŸ“ ä¿å­˜ç›®å½•: {os.path.abspath(export_dir)}/")

            # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
            print(f"\nğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶:")
            csv_files = [f for f in os.listdir(export_dir) if f.endswith('.csv')]
            for file in sorted(csv_files):
                filepath = f"{export_dir}/{file}"
                try:
                    # ç®€å•ç»Ÿè®¡è¡Œæ•°
                    with open(filepath, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        count = len(lines) - 1 if len(lines) > 0 else 0
                    print(f"  {file}: {count:,} æ¡")
                except Exception as e:
                    print(f"  {file}: è¯»å–å¤±è´¥ ({e})")

        except Exception as e:
            print(f"\nâŒ å¯¼å‡ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    def _export_to_csv_safe(self, data: Any, filename: str) -> bool:
        """
        å®‰å…¨çš„å¯¼å‡ºæ•°æ®åˆ°CSVï¼Œè¿”å›æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
            if data is None:
                print(f"  âš ï¸  æ•°æ®ä¸ºç©º: {filename}")
                return False

            # å¦‚æœæ˜¯å­—å…¸ï¼Œè½¬ä¸ºåˆ—è¡¨
            if isinstance(data, dict):
                data = [data]

            # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ—è¡¨
            if not isinstance(data, list):
                print(f"  âŒ æ•°æ®ä¸æ˜¯åˆ—è¡¨: {type(data)}")
                return False

            # æ£€æŸ¥åˆ—è¡¨æ˜¯å¦ä¸ºç©º
            if len(data) == 0:
                print(f"  âš ï¸  æ•°æ®åˆ—è¡¨ä¸ºç©º: {filename}")
                return False

            # æ£€æŸ¥åˆ—è¡¨ä¸­çš„å…ƒç´ 
            valid_data = []
            for i, item in enumerate(data):
                if item is None:
                    continue
                if not isinstance(item, dict):
                    continue
                valid_data.append(item)

            if not valid_data:
                print(f"  âš ï¸  æ— æœ‰æ•ˆæ•°æ®: {filename}")
                return False

            # è·å–æ‰€æœ‰å­—æ®µ
            all_fields = set()
            for item in valid_data:
                if isinstance(item, dict):
                    all_fields.update(item.keys())

            if not all_fields:
                print(f"  âš ï¸  æ— æœ‰æ•ˆå­—æ®µ: {filename}")
                return False

            # å¯¼å‡ºCSV
            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=sorted(all_fields))
                writer.writeheader()
                writer.writerows(valid_data)

            print(f"  âœ… å·²å¯¼å‡º: {filename} ({len(valid_data):,} æ¡)")
            return True

        except Exception as e:
            print(f"  âŒ å¯¼å‡ºå¤±è´¥ {filename}: {e}")
            return False

    def show_config(self):
        """æ˜¾ç¤ºå½“å‰é…ç½®"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ æ•°æ®è·å–é…ç½®")
        print("=" * 60)

        total_target = sum(self.config.values())
        print(f"ğŸ¯ ç›®æ ‡æ€»æ•°æ®æ¡æ•°: {total_target:,}")
        print("\nå„æ•°æ®ç±»å‹ç›®æ ‡:")
        for key, value in self.config.items():
            print(f"  â€¢ {key}: {value:,} æ¡")

        print(f"\nğŸ’¡ æç¤º:")
        print(f"  1. å½“å‰é…ç½®è¾ƒä¸ºä¿å®ˆï¼Œé¿å…è§¦å‘APIé™åˆ¶")
        print(f"  2. å¦‚éœ€ä¿®æ”¹æ•°é‡ï¼Œå¯ç›´æ¥ç¼–è¾‘configå­—å…¸")
        print(f"  3. æ‰€æœ‰æ•°æ®è·å–éƒ½ç»è¿‡å®‰å…¨å¤„ç†")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ VS Code GitHubä»“åº“å¤§æ•°æ®çˆ¬è™«ï¼ˆå®‰å…¨ä¿®å¤ç‰ˆï¼‰")
    print("=" * 60)
    print("âœ… ä¸“é—¨ä¿®å¤NoneTypeé”™è¯¯å’ŒAPIé™åˆ¶é—®é¢˜")
    print("=" * 60)

    # å¿…é¡»è®¾ç½®ä½ çš„GitHub Token
    GITHUB_TOKEN = "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1"  # å¿…é¡»æ›¿æ¢ï¼

    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = MaxDataVSCodeCrawler(github_token=GITHUB_TOKEN)

        # æ˜¾ç¤ºé…ç½®
        crawler.show_config()

        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        print("1. å¯¼å‡ºæ‰€æœ‰æ•°æ®ï¼ˆæ¨èï¼‰")
        print("2. åªæµ‹è¯•å•ä¸ªAPI")
        print("3. è‡ªå®šä¹‰é…ç½®")

        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()

        if choice == "1":
            export_dir = input("è¾“å…¥å¯¼å‡ºç›®å½•å (é»˜è®¤: vscode_massive_data): ").strip()
            if not export_dir:
                export_dir = "vscode_massive_data"

            confirm = input(f"å°†å¯¼å‡ºåˆ° {export_dir}/ï¼Œç¡®è®¤å¼€å§‹ï¼Ÿ(y/n): ").strip().lower()
            if confirm == 'y':
                print(f"\nğŸš€ å¼€å§‹è·å–æ•°æ®ï¼Œè¯·è€å¿ƒç­‰å¾…...")
                print("ç¨‹åºä¼šè‡ªåŠ¨å¤„ç†APIé™åˆ¶ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
                print("-" * 60)
                crawler.export_massive_data_safely(export_dir)
            else:
                print("æ“ä½œå·²å–æ¶ˆ")

        elif choice == "2":
            print("\nå¯é€‰æµ‹è¯•çš„API:")
            print("1. è´¡çŒ®è€… API")
            print("2. æäº¤è®°å½• API")
            print("3. é—®é¢˜ API")
            print("4. PR API")

            api_choice = input("é€‰æ‹©è¦æµ‹è¯•çš„API (1-4): ").strip()

            if api_choice == "1":
                print("\næµ‹è¯•è´¡çŒ®è€…API...")
                data = crawler.get_massive_contributors()
                print(f"è·å–åˆ° {len(data) if data else 0} æ¡æ•°æ®")

            elif api_choice == "2":
                print("\næµ‹è¯•æäº¤è®°å½•API...")
                data = crawler.get_massive_commits()
                print(f"è·å–åˆ° {len(data) if data else 0} æ¡æ•°æ®")

            elif api_choice == "3":
                print("\næµ‹è¯•é—®é¢˜API...")
                data = crawler.get_massive_issues_safe(state="open", issue_type="issues")
                print(f"è·å–åˆ° {len(data) if data else 0} æ¡æ•°æ®")

            elif api_choice == "4":
                print("\næµ‹è¯•PR API...")
                data = crawler.get_massive_issues_safe(state="open", issue_type="pulls")
                print(f"è·å–åˆ° {len(data) if data else 0} æ¡æ•°æ®")

        elif choice == "3":
            print("\nå½“å‰é…ç½®:")
            for i, (key, value) in enumerate(crawler.config.items(), 1):
                print(f"{i}. {key}: {value:,}")

            config_idx = input("\nè¾“å…¥è¦ä¿®æ”¹çš„é…ç½®ç¼–å· (æˆ–æŒ‰Enterè·³è¿‡): ").strip()
            if config_idx:
                try:
                    idx = int(config_idx) - 1
                    keys = list(crawler.config.keys())
                    if 0 <= idx < len(keys):
                        key = keys[idx]
                        new_value = input(f"è¯·è¾“å…¥æ–°çš„ {key} æ•°é‡ (å½“å‰: {crawler.config[key]:,}): ").strip()
                        if new_value:
                            crawler.config[key] = int(new_value)
                            print(f"âœ… {key} å·²æ›´æ–°ä¸º {crawler.config[key]:,}")
                except Exception as e:
                    print(f"âŒ ä¿®æ”¹å¤±è´¥: {e}")

            export_dir = "vscode_custom_data"
            confirm = input(f"ä½¿ç”¨æ–°é…ç½®å¯¼å‡ºåˆ° {export_dir}/ï¼Ÿ(y/n): ").strip().lower()
            if confirm == 'y':
                crawler.export_massive_data_safely(export_dir)

    except ValueError as e:
        print(f"âŒ {e}")
        print("è¯·ç¡®ä¿å·²è®¾ç½®æ­£ç¡®çš„GitHub Token")
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # å®‰è£…ä¾èµ–: pip install requests pandas
    main()